---
title: "ANN Benchmarks"
author: "Amos Elberg"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ANN Benchmarks}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setupbenchmark,eval=T,echo=F,warning=F,error=F,message=F}
# Note to reader:  Please don't steal the semi-distinctive visual style I spent several minutes creating for myself.
require(ggplot2, 
        quietly = TRUE)
require(RColorBrewer, 
        quietly = TRUE)
require(wesanderson, 
        quietly = TRUE)
require(dplyr, quietly = TRUE)
knitr::opts_chunk$set(collapse = TRUE, 
                      comment = "#>")
colors_discrete <- function(x) rep(wes_palette("Darjeeling", 
                                               n = min(x, 5)), 
                                   2)[1:x]
colors_divergent_discrete <- function(x) 
  grDevices::colorRampPalette(RColorBrewer::brewer.pal(x, "Spectral"))
colors_continuous <-  function(x) wes_palette(name = "Zissou",
                                              n = x, 
                                              type = "continuous")

nacol <- colors_discrete(4)[4]
theme_set(
  theme_bw() %+replace%
  theme(
    legend.key.size = unit(4, "mm"), 
    legend.title = element_text(size = rel(0.8),
                              face = "bold"),
    legend.margin = unit(0, "cm"),
    legend.position = "bottom",
    legend.key.size = unit(0.5, "lines"),
    legend.text=element_text(size = unit(8, "points")), 
    axis.title.y = element_text(angle = 90),
    axis.text = element_text(size = rel(0.7)),
    plot.margin = unit(c(0, 0.5, 1, 0), "lines"), 
    axis.title = element_text(size = rel(0.8),
                              face = "bold"),
    title = element_text(size = rel(0.9))
  ) 
)
rebuild <- FALSE
if (!exists("buildManifolds")) buildManifolds <- rebuild
```

## Overview

Besides manifold visualization, `largeVis` also includes an extremely efficient approximate nearest-neighbor search that runs in $O(n)$ time. 

This vignette includes benchmarks and recommendations for adjusting hyperparameters in the neighbor search for best results. 

## Hyperparameters

The `randomProjectionTreeSearch` function has three hyperparameters that trade-off accuracy and efficiency in the neighbor search:

1.  `n_trees` - In the first phase of the function, the number of random projection trees to create.
2.  `tree_threshold` - The maximum number of any nodes on a random projection tree leaf. If, after branching, the number of nodes in a branch exceeds this threshold, the branch will be divided again. 
3.  `max_iters` - The number of iterations for the neighborhood-exploration phase of the algorithm.

## Data Collection \& Methodology

The data in the benchmarks below was obtained by running the `benchmark.R` script, which is installed along with the package, on two machines.  

The aim was to replicate as much as possible the methodology used by Erik Bernhardsson's [ANN Benchmark](https://github.com/erikbern/ann-benchmarks) github.  It is not possible to use the same methods exactly.  This is because `ANN Benchmark` is designed for libraries that are designed to build a neighbor index and then rapidly process queries against the index. The measure used by `ANN Benchmark` is therefore queries-per-second.  By contract, `largeVis` is concerned with getting neighbors for all of the nodes in a finite dataset as quickly as possible. 

The data used is the 1-million vector, 128-feature [SIFT Dataset](http://corpus-texmex.irisa.fr/), which is the test data used by `ANN Benchmark`. 

Resulting times were normalized for each machine against the time for `RcppAnnoy` to identify neighbors for 10,000 vectors using 10 trees. 

Results that appear to have used virtual memory, in that the completion time was radically discontinuous with other results from the same machine, were discarded. 

I welcome submissions of output from the script from other hardware. 

## Comparison With Annoy

The following chart illustrates performance versus the `Annoy` library, as implemented through the `RcppAnnoy` R package.

```{r performance,echo=F,eval=rebuild}
# benchmark <- readr::read_csv(system.file("extdata", "results.csv", package="largeVis"), 
#                              col_names = FALSE)
benchmark <- readr::read_csv("../inst/results.csv", col_names = FALSE)
benchmark$machine <- 1
benchmark2 <- readr::read_csv("../inst/nelsonresults.csv", col_names = FALSE)
benchmark2$machine <- 2

benchmark <- rbind(benchmark, benchmark2) %>% 
  colnames(c("time",
    "precision",
    "n_trees",
    "max_iters",
    "threshold",
    "method",
    "tree_type",
    "search_type",
    "eps", 
    "machine")
    ) %>%
  select(-tree_type, -search_type, - eps) %>% 
  mutate(method <- ifelse(grep("largeVis", method), "largeVis", method), 
         series = ifelse(grep("largeVis", method), 
                         paste(method, n_trees), 
                         method), 
         series = factor(series), 
         method = factor(method)) %>%
  group_by(machine) %>%
  mutate(time = time / min(time))

#benchmark[(benchmark$n_trees == 10 & benchmark$method == 'RcppAnnoy'),]

# benchmark$time <- benchmark$time / ifelse(benchmark$machine == 1, 1.444920, 1.535254)

```


```{r plotpeformance,echo=F,fig.width=5,fig.height=6,fig.align='center',warning=FALSE,message=FALSE}
load(system.file("extdata", "benchmark.Rda", package = "largeVis"))
ggplot(benchmark[benchmark$series != 'RcppAnnoy',], aes(x = time, 
                      y = 1 - (precision / 100), 
                      group = series, 
                      color = series, 
                      shape = series,
                      label = labels)) +
  geom_point(size = 2) + 
 # geom_line(size = 0.5) + 
  # geom_text(size = 3, 
  #           nudge_x = 10) +
  scale_x_log10(name = "log(time) Relative to Annoy w/ 10 Trees") + 
  scale_y_sqrt("Error", 
                limits = c(0.01,1), 
                breaks = 1 - c(.1, .25, .5, .8, .9, .99)) +
  scale_color_manual(values =      colors_divergent_discrete(nlevels(benchmark$series))(nlevels(benchmark$series))) +
  guides(color = guide_legend(nrow=3)) +
  ggtitle(expression(
    atop("Time vs. Error Rate (K = 100, n = 10000)",
         atop(italic("(Lower Left is Better)"))
         )
    ))
```

This plot shows times for `RcppAnnoy` to find neighbors for all vertices in the dataset.  

## Effect of Increasing Number of Trees

```{r max_iters,echo=F}
benchmark %>%
  filter(method == 'largeVis') %>%
  mutate(series = paste(machine, threshold, max_iters), 
         series = factor(series)) %>%
  select(-threshold, -max_iters, -method, -labels, -machine) %>%
  group_by(series) %>% 
  filter(n() > 1) %>%
  ungroup() %>%
  mutate(series = factor(series)) %>%
  group_by(series) %>%
  arrange(n_trees) %>%
  mutate(error = 1 - (precision / 100)) %>%
  ggplot(aes(x = time, y = error, color = series, group = series)) + 
  geom_line(arrow = arrow(length = unit(0.05, "inches"))) +
  geom_point() +
  scale_x_log10(name = "log(time) Relative to Annoy w/ 10 Trees") + 
  scale_y_sqrt("Error", 
                limits = c(0.01,1), 
                breaks = 1 - c(.1, .25, .5, .8, .9, .99)) +
  scale_color_manual(values =      colors_divergent_discrete(nlevels(benchmark$series))(nlevels(benchmark$series))) +
  ggtitle(expression(
    atop("Time vs. Error Rate (K = 100, n = 10000)",
         atop(italic("(Lower Left is Better)"))
         )
    ))
```

## Effect of Increasing The Tree Threshold

```{r max_iters,echo=F}
benchmark %>%
  filter(method == 'largeVis') %>%
  mutate(series = paste(machine, n_trees, max_iters), 
         series = factor(series)) %>%
  select(-n_trees, -max_iters, -method, -labels, -machine) %>%
  group_by(series) %>% 
  filter(n() > 1) %>%
  ungroup() %>%
  mutate(series = factor(series)) %>%
  group_by(series) %>%
  arrange(threshold) %>%
  mutate(error = 1 - (precision / 100)) %>%
  ggplot(aes(x = time, y = error, color = series, group = series)) + 
  geom_line(arrow = arrow(length = unit(0.05, "inches"))) +
  geom_point() +
  scale_x_log10(name = "log(time) Relative to Annoy w/ 10 Trees") + 
  scale_y_sqrt("Error", 
                limits = c(0.01,1), 
                breaks = 1 - c(.1, .25, .5, .8, .9, .99)) +
  scale_color_manual(values =      colors_divergent_discrete(nlevels(benchmark$series))(nlevels(benchmark$series))) +
  ggtitle(expression(
    atop("Time vs. Error Rate (K = 100, n = 10000)",
         atop(italic("(Lower Left is Better)"))
         )
    ))
```

## Effect of Increasing The Number of Iterations

```{r max_iters,echo=F}
benchmark %>%
  filter(method == 'largeVis') %>%
  mutate(series = paste(machine, n_trees, threshold), 
         series = factor(series)) %>%
  select(-n_trees, -threshold, -method, -labels, -machine) %>%
  group_by(series) %>% 
  filter(n() > 1) %>%
  ungroup() %>%
  mutate(series = factor(series)) %>%
  group_by(series) %>%
  arrange(max_iters) %>%
  mutate(error = 1 - (precision / 100)) %>%
  ggplot(aes(x = time, y = error, color = series, group = series)) + 
  geom_line(arrow = arrow(length = unit(0.05, "inches"))) +
  geom_point() +
  scale_x_log10(name = "log(time) Relative to Annoy w/ 10 Trees") + 
  scale_y_sqrt("Error", 
                limits = c(0.01,1), 
                breaks = 1 - c(.1, .25, .5, .8, .9, .99)) +
  ggtitle(expression(
    atop("Time vs. Error Rate (K = 100, n = 10000)",
         atop(italic("(Lower Left is Better)"))
         )
    ))
```

The data shows a consistent pattern where adding a single iteration has, a substantial impact on accuracy, but the marginal benefit of additional iterations is low.  This is consistent with the recommendation of the paper authors'.
