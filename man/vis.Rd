% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/largeVis.R
\name{vis}
\alias{vis}
\title{Apply the LargeVis algorithm for visualizing large high-dimensional datasets.}
\usage{
vis(x, dim = 2, K = 40, n_trees = 50, tree_threshold = max(10, ncol(x)),
  max_iter = 1, max_depth = 32, distance_method = "Euclidean",
  perplexity = 50, sgd_batches = ncol(x) * 20000, M = 5,
  weight_pos_samples = TRUE, alpha = 1, gamma = 7, rho = 1,
  min_rho = 0, coords = NULL, save_neighbors = TRUE,
  save_sigmas = FALSE, verbose = TRUE, ...)
}
\arguments{
\item{x}{A matrix, where the features are rows and the examples are columns.}

\item{dim}{The number of dimensions in the output}

\item{K}{The number of nearest-neighbors to use in computing the kNN graph}

\item{n_trees}{See \code{\link{randomProjectionTreeSearch}}.  The default is set at 50, which is the number
used in the examples in the original paper.}

\item{tree_threshold}{See \code{\link{randomProjectionTreeSearch}}.  By default, this is the number of features
in the input set, which is the setting used in the examples in the original paper.  Note the time and memory requirements:
the first pass through the neighborhood exploration phases will involve up to \eqn{N * nTrees * threshold} comparisons.}

\item{max_iter}{See \code{\link{randomProjectionTreeSearch}}.}

\item{max_depth}{See \code{\link{randomProjectionTreeSearch}}}

\item{distance_method}{One of "Euclidean" or "Cosine."  See \code{\link{randomProjectionTreeSearch}}.}

\item{perplexity}{See paper}

\item{sgd_batches}{See \code{\link{projectKNNs}}.}

\item{M}{See \code{\link{projectKNNs}}.}

\item{weight_pos_samples}{See \code{\link{projectKNNs}}.}

\item{alpha}{See \code{\link{projectKNNs}}.}

\item{gamma}{See \code{\link{projectKNNs}}.}

\item{rho}{See \code{\link{projectKNNs}}.}

\item{min_rho}{\code{\link{projectKNNs}}.}

\item{coords}{A [N,K] matrix of coordinates to use as a starting point -- useful for refining an embedding in stages.}

\item{save_neighbors}{Whether to include in the output the adjacency matrix of nearest neighbors.}

\item{save_sigmas}{Whether to include in the output the esimates values of sigma.}

\item{verbose}{Verbosity}

\item{...}{See paper}
}
\value{
A `largeVis` object with the following slots:
 \describe{
   \item{'knns'}{An [N,K] 0-indexed integer matrix, which is an adjacency list of each vertex' identified nearest neighbors.
   If the algorithm failed to find \code{K} neighbors, the matrix is padded with \code{NA}'s.}
   \item{'wij'}{A sparse [N,N] matrix where each cell represents \eqn{w_{ij}}.}
   \item{'call'}{The call.}
   \item{'coords'}{A [N,D] matrix of the embedding of the dataset in the low-dimensional space.}
   \item{'sigmas'}{A [N] vector of the values of sigma estimated for each vertex. Primarily useful for debugging
   purposes and therefore not returned by default.}
 }
}
\description{
Implements the \code{vis}
}
\details{
Note that this implementation expects the data to be free of \code{NaN}'s, \code{NA}'s, \code{Inf}'s, and duplicate rows.
If any of these assumptions are violated, the algorithm will fail. It is also usually a good idea to scale the input data
to have unit norm and mean 0. If there are large values in the input matrix, some computations may oveflow.
}
\examples{
# iris
data(iris)
dat <- as.matrix(iris[,1:4])
dat <- scale(dat)
dupes = which(duplicated(dat))
dat <- dat[-dupes,] # duplicated data potentially can cause the algorithm to fail
dat <- t(dat)
visObject <- vis(dat, max_iter = 20, sgd_batches = 800000,
                     K = 10,  gamma = 2, rho = 1, M = 40, alpha = 20,verbose=FALSE)
\dontrun{
# mnist
load("./mnist.Rda")
dat <- mnist$images
dim(dat) <- c(42000, 28 * 28)
dat <- (dat / 255) - 0.5
dat <- t(dat)
coords <- vis(dat, check=FALSE,
             n_tree = 50, tree_th = 200,
             K = 50, alpha = 2, max.iter = 4)
}

}
\references{
Jian Tang, Jingzhou Liu, Ming Zhang, Qiaozhu Mei. \href{https://arxiv.org/abs/1602.00370}{Visualizing Large-scale and High-dimensional Data.}
}

